{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "autograding-using-lstm-tf-keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushkal1234/Automatic-Essay-Grading_NLP/blob/main/automatic%20essay%20grading%20NLP_lstm_tf_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "dQOMYjEDuPru"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For edfample, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For edfample, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "        \n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_7N4fCEOuPrv",
        "outputId": "06537905-93c6-46ec-badf-2c367f198474"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/input/train.csv\")\n",
        "y = df['evaluator_rating']\n",
        "df.drop(df.columns[[0]],axis=1, inplace= True)\n",
        "df = df.dropna(axis=1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>promptId</th>\n",
              "      <th>uniqueId</th>\n",
              "      <th>essay</th>\n",
              "      <th>evaluator_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1_323</td>\n",
              "      <td>At present age, our education system is not go...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1_238</td>\n",
              "      <td>I am agree the tightly defined curriculum of o...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1_212</td>\n",
              "      <td>I strongly agree with the statement that tight...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1_117</td>\n",
              "      <td>Our education system is nice quitely but i dis...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1_229</td>\n",
              "      <td>i am totally agree with the statement that tig...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   promptId  ... evaluator_rating\n",
              "0         1  ...              3.0\n",
              "1         1  ...              4.0\n",
              "2         1  ...              2.0\n",
              "3         1  ...              2.0\n",
              "4         1  ...              3.0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "IwmiAFo1uPrw",
        "outputId": "f876999c-cd3c-44cf-b880-a6be5f1668b0"
      },
      "source": [
        "df['essay'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I am agree the tightly defined curriculum of our education system leaves no room for imagination and creativity.A hidden curriculum can be defined as the lesson that are taught informally,and usually unintentionally in the school and colleges. These include behaviour ,perspectives and attitudes that students pick up while they are at school.This is contrasted with the formal curriculum,such as the courses and activities students participate in,The curriculum began early in the child's education. Students learn to form opinion and ideas about their environment and their classmate.They also learn what is expected of them.These attitudes and ideas are not taught in formal way,but kids absorb them through natural observation and participation in classroom more and more and also participate in social activities.Areas of hidden curriculum in our school that mold perspectives of students deal with issues such as gender,morals,social class,politics and language.Many books and magzine which are related to expected nature at this young age support the idea of today's sepration which are encourage the youths.The importance of boy athletes used to be clear example of hidden curriculum.Student imagination and creativity are also developed by some other activities.Education is the process of facilitating learning,kowledge,skills,belief and habits.Education can take place in formal or informal setting an any experience that has a formative effect on the way one think,feels and acts may be considered educational.Education is more important for every students and children.The influence of this can lead to a negative self image or hatred for reading.These researchers discuss how the student increase the knowledge related to nature and many others areas.Their goal has been to raise the general awareness of hidden education curriculum in our school in order to make education system progressive and accessible to students of all culture ,linguistic and many other areeas which are related to ediucation system.The\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wURR_fPBJqtl"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")                     #Ignoring unnecessory warnings\n",
        "\n",
        "import numpy as np                                  #for large and multi-dimensional arrays\n",
        "import pandas as pd                                 #for data manipulation and analysis\n",
        "import nltk                                         #Natural language processing tool-kit\n",
        "\n",
        "from nltk.corpus import stopwords                   #Stopwords corpus\n",
        "from nltk.stem import PorterStemmer                 # Stemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\n",
        "from gensim.models import Word2Vec                                   #For Word2Vec\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nHOZZwL-uPrw"
      },
      "source": [
        "list_1 = []\n",
        "list_2 = []\n",
        "list_3 = []\n",
        "list_4 = []\n",
        "list_5 = []\n",
        "list_6 = []\n",
        "list_7 = []\n",
        "list_8 = []\n",
        "list_9 = []\n",
        "list_10 = []\n",
        "list_11 = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eERRDkBBuPrx"
      },
      "source": [
        "data = []\n",
        "num = [0,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n",
        "count = 0\n",
        "for i,j in zip(range(len(df)),df[\"essay\"]):\n",
        "    if df[\"evaluator_rating\"][i] == 0:\n",
        "        list_1.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 0.5:\n",
        "        list_2.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 1:\n",
        "        list_3.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 1.5:\n",
        "        list_4.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 2:\n",
        "        list_5.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 2.5:\n",
        "        list_6.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 3:\n",
        "        list_7.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 3.5:\n",
        "        list_8.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 4:\n",
        "        list_9.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 4.5:\n",
        "        list_10.append(j)\n",
        "    if df[\"evaluator_rating\"][i] == 5:\n",
        "        list_11.append(j)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NXxJztQuPrx",
        "outputId": "23d14681-79e8-4789-9d08-0d8cf47570d6"
      },
      "source": [
        "import itertools\n",
        "data = list(itertools.chain(list_1,list_2,list_3,list_4,list_5,list_6,list_7,list_8,list_9,list_10,list_11))\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1240"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cKK9-HOcuPry"
      },
      "source": [
        "def gen_num(num,length):\n",
        "    num_list = [num]*length\n",
        "    return num_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7lQC5SduPry",
        "outputId": "e20721ed-9202-4b72-f03d-d27f27dfaab8"
      },
      "source": [
        "score_1 = gen_num(0,len(list_1))\n",
        "score_2 = gen_num(0.5,len(list_2))\n",
        "score_3 = gen_num(1,len(list_3))\n",
        "score_4 = gen_num(1.5,len(list_4))\n",
        "score_5 = gen_num(2,len(list_5))\n",
        "score_6 = gen_num(2.5,len(list_6))\n",
        "score_7 = gen_num(3,len(list_7))\n",
        "score_8 = gen_num(3.5,len(list_8))\n",
        "score_9 = gen_num(4,len(list_9))\n",
        "score_10 = gen_num(4.5,len(list_10))\n",
        "score_11 = gen_num(5,len(list_11))\n",
        "\n",
        "score = list(itertools.chain(score_1,score_2,score_3,score_4,score_5,score_6,score_7,score_8,score_9,score_10,score_11))\n",
        "len(score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1240"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "GHyGUqxxuPry",
        "outputId": "c1f5c7e2-28c1-4ef3-be04-9ed638d47895"
      },
      "source": [
        "# dictionary of lists \n",
        "dictnary = {'essay': data, 'score': score}     \n",
        "df = pd.DataFrame(dictnary) \n",
        "df "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Education is the most important part of our li...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Our education inindia was good,but not that mu...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Education is very important in each and every ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>education system is the most important for us ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>today's eduction system is very bad due to res...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1235</th>\n",
              "      <td>Imagination is the mother of invention and wit...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236</th>\n",
              "      <td>Just learning a text-book by heart and getting...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1237</th>\n",
              "      <td>YES I AGREE WITH THIS STATEMENT THAT THE TIGHT...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>I speak about education from an unflattering p...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239</th>\n",
              "      <td>Yes, it is true that the gap between rich and ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1240 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  essay  score\n",
              "0     Education is the most important part of our li...    0.0\n",
              "1     Our education inindia was good,but not that mu...    0.0\n",
              "2     Education is very important in each and every ...    0.0\n",
              "3     education system is the most important for us ...    0.0\n",
              "4     today's eduction system is very bad due to res...    0.0\n",
              "...                                                 ...    ...\n",
              "1235  Imagination is the mother of invention and wit...    5.0\n",
              "1236  Just learning a text-book by heart and getting...    5.0\n",
              "1237  YES I AGREE WITH THIS STATEMENT THAT THE TIGHT...    5.0\n",
              "1238  I speak about education from an unflattering p...    5.0\n",
              "1239  Yes, it is true that the gap between rich and ...    5.0\n",
              "\n",
              "[1240 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCuO0CpRuPrz",
        "outputId": "e679b10b-5e27-4fbb-baef-031b161d6b21"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsCHybZvuPrz",
        "outputId": "4d79de7b-c1e8-48db-eb62-2591748f6584"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "len(stop_words) #finding stop words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CbXLMSjsuPrz"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "snow = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "corpus = []\n",
        "for i in range(0, len(df)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', df['essay'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = [snow.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kaqc764RuPrz",
        "outputId": "3d397876-219e-42dd-bd13-cafbc6cedee1"
      },
      "source": [
        "voc_size=5000\n",
        "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
        "type(onehot_repr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dHeKBBtuPr0",
        "outputId": "c181be1d-bbac-46dd-b8a5-d34f75a08c3c"
      },
      "source": [
        "sent_length=400\n",
        "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
        "print(embedded_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ... 4385 1260  908]\n",
            " [   0    0    0 ... 1317 1726 3464]\n",
            " [   0    0    0 ... 4278 2529 2028]\n",
            " ...\n",
            " [   0    0    0 ... 1335 1060 2529]\n",
            " [   0    0    0 ... 1251    9  602]\n",
            " [   0    0    0 ... 4109 1974 1752]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gD0YIxduPr0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7lpd9iD3uPr0"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def essay_to_wordlist(essay_v, remove_stopwords):\n",
        "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
        "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
        "    words = essay_v.lower().split()\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return (words)\n",
        "\n",
        "def essay_to_sentences(essay_v, remove_stopwords):\n",
        "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences\n",
        "\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            featureVec = np.add(featureVec,model[word])        \n",
        "    featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec\n",
        "\n",
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcDMEI0puPr1",
        "outputId": "83fb4fd8-ba2e-48a7-df7f-8891b1fe095d"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P4t59QDfuPr1"
      },
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy','mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4r6KuH2vuPr1"
      },
      "source": [
        "df=df\n",
        "y = df['score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCU3sGKh0N9J",
        "outputId": "03af39db-5503-4cc2-d923-94ccea96bc9d"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jdBL98GwuPr2",
        "outputId": "e2d6cad2-5215-48da-d237-2a2a1ea5de2d"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cv = KFold(n_splits = 5, shuffle = True)\n",
        "results = []\n",
        "y_pred_list = []\n",
        "\n",
        "count = 1\n",
        "for traincv, testcv in cv.split(df):\n",
        "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
        "    df_test, df_train, y_test, y_train = df.iloc[testcv], df.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
        "    \n",
        "    train_essays = df_train['essay']\n",
        "    test_essays = df_test['essay']\n",
        "    \n",
        "    sentences = []\n",
        "    \n",
        "    for essay in train_essays:\n",
        "            # Obtaining all sentences from the training essays.\n",
        "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
        "            \n",
        "    # Initializing variables for word2vec model.\n",
        "    num_features = 300 \n",
        "    min_word_count = 40\n",
        "    num_workers = 4\n",
        "    context = 10\n",
        "    downsampling = 1e-3\n",
        "\n",
        "    print(\"Training Word2Vec Model...\")\n",
        "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
        "\n",
        "    model.init_sims(replace=True)\n",
        "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
        "\n",
        "    clean_train_essays = []\n",
        "    \n",
        "    # Generate training and testing data word vectors.\n",
        "    for essay_v in train_essays:\n",
        "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
        "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
        "    \n",
        "    clean_test_essays = []\n",
        "    for essay_v in test_essays:\n",
        "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
        "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "    \n",
        "    trainDataVecs = np.array(trainDataVecs)\n",
        "    testDataVecs = np.array(testDataVecs)\n",
        "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
        "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
        "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "    \n",
        "    lstm_model = get_model()\n",
        "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
        "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
        "    y_pred = lstm_model.predict(testDataVecs)\n",
        "    \n",
        "    # Save any one of the 5 models.\n",
        "    if count == 5:\n",
        "         lstm_model.save('./final_lstm.h5')\n",
        "    \n",
        "    # Round y_pred to the nearest integer.\n",
        "    y_pred = np.around(y_pred)\n",
        "    \n",
        "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
        "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    acc = accuracy_score(y_test.values,y_pred)\n",
        "    print(\"acc Score: {}\".format(acc))\n",
        "    print(\"Kappa Score: {}\".format(result))\n",
        "    results.append(result)\n",
        "\n",
        "    count += 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------Fold 1--------\n",
            "\n",
            "Training Word2Vec Model...\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_60 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_61 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 8s 26ms/step - loss: 6.1970 - accuracy: 0.0601 - mae: 2.2267\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.3187 - accuracy: 0.0282 - mae: 0.8872\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2046 - accuracy: 0.0280 - mae: 0.8386\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.4313 - accuracy: 0.0314 - mae: 0.9157\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2994 - accuracy: 0.0226 - mae: 0.8646\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2581 - accuracy: 0.0228 - mae: 0.8559\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2756 - accuracy: 0.0357 - mae: 0.8674\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 1.2198 - accuracy: 0.0306 - mae: 0.8350\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 1.2628 - accuracy: 0.0356 - mae: 0.8711\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.2162 - accuracy: 0.0239 - mae: 0.8443\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 1.1980 - accuracy: 0.0231 - mae: 0.8424\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 1.2373 - accuracy: 0.0268 - mae: 0.8557\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2688 - accuracy: 0.0332 - mae: 0.8606\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2049 - accuracy: 0.0273 - mae: 0.8384\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.1759 - accuracy: 0.0249 - mae: 0.8289\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.2436 - accuracy: 0.0322 - mae: 0.8381\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2795 - accuracy: 0.0291 - mae: 0.8567\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2068 - accuracy: 0.0350 - mae: 0.8382\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2710 - accuracy: 0.0282 - mae: 0.8529\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2745 - accuracy: 0.0273 - mae: 0.8637\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 1.2843 - accuracy: 0.0294 - mae: 0.8630\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.1781 - accuracy: 0.0314 - mae: 0.8265\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.3047 - accuracy: 0.0321 - mae: 0.8953\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2726 - accuracy: 0.0221 - mae: 0.8678\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.2416 - accuracy: 0.0330 - mae: 0.8522\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.1831 - accuracy: 0.0309 - mae: 0.8227\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.1483 - accuracy: 0.0234 - mae: 0.7942\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2142 - accuracy: 0.0244 - mae: 0.8232\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.1940 - accuracy: 0.0257 - mae: 0.8212\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 1.2597 - accuracy: 0.0202 - mae: 0.8584\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 1.2178 - accuracy: 0.0260 - mae: 0.8417\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2210 - accuracy: 0.0284 - mae: 0.8390\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 1.1623 - accuracy: 0.0288 - mae: 0.8204\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2549 - accuracy: 0.0341 - mae: 0.8610\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.1796 - accuracy: 0.0320 - mae: 0.8173\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.2443 - accuracy: 0.0300 - mae: 0.8320\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2386 - accuracy: 0.0303 - mae: 0.8535\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.1299 - accuracy: 0.0292 - mae: 0.8029\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2537 - accuracy: 0.0251 - mae: 0.8499\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2479 - accuracy: 0.0284 - mae: 0.8342\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2060 - accuracy: 0.0329 - mae: 0.8491\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2328 - accuracy: 0.0300 - mae: 0.8486\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.2499 - accuracy: 0.0273 - mae: 0.8506\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2350 - accuracy: 0.0326 - mae: 0.8326\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.1629 - accuracy: 0.0268 - mae: 0.8275\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.2464 - accuracy: 0.0255 - mae: 0.8487\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2859 - accuracy: 0.0276 - mae: 0.8567\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2617 - accuracy: 0.0312 - mae: 0.8671\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2598 - accuracy: 0.0281 - mae: 0.8600\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.1780 - accuracy: 0.0341 - mae: 0.8269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-185-12d6fe8b8333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohen_kappa_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'quadratic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mcohen_kappa_score\u001b[0;34m(y1, y2, labels, weights, sample_weight)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     confusion = confusion_matrix(y1, y2, labels=labels,\n\u001b[0;32m--> 585\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0msum0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_D5nWbouPr2",
        "outputId": "3843980c-0e21-4ac2-93b5-8044dc5eac0e"
      },
      "source": [
        "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Kappa score after a 5-fold cross validation:  0.1147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "TmqITwgyuPr2",
        "outputId": "aa995956-2a13-4a17-bf7a-11c0fc078ae3"
      },
      "source": [
        "demo = [\"Dear@CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many people have myspace, facebooks, aim, these all benefit us by having conversations with one another. Many people believe computers are bad but how can you make friends if you can never talk to them? I am very fortunate for having a computer that can help with not only school work but my social life and how I make friends. Computers help us with finding our locations, coordibates and millions of information online. If we didn't go on the internet a lot we wouldn't know how to go onto websites that @MONTH1 help us with locations and coordinates like @LOCATION1. Would you rather use a computer or be in @LOCATION3. When your supposed to be vacationing in @LOCATION2. Million of information is found on the internet. You can as almost every question and a computer will have it. Would you rather easily draw up a house plan on the computers or take @NUM1 hours doing one by hand with ugly erazer marks all over it, you are garrenteed that to find a job with a drawing like that. Also when appling for a job many workers must write very long papers like a @NUM3 word essay on why this job fits you the most, and many people I know don't like writing @NUM3 words non-stopp for hours when it could take them I hav an a computer. That is why computers we needed a lot now adays. I hope this essay has impacted your descion on computers because they are great machines to work with. The other day I showed my mom how to use a computer and she said it was the greatest invention sense sliced bread! Now go out and buy a computer to help you chat online with friends, find locations and millions of information on one click of the button and help your self with getting a job with neat, prepared, printed work that your boss will love.\"]\n",
        "demo_df = pd.DataFrame(demo,columns=['essay'])\n",
        "demo_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear@CAPS1 @CAPS2, I believe that using comput...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               essay\n",
              "0  Dear@CAPS1 @CAPS2, I believe that using comput..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpOIFNK1uPr3",
        "outputId": "963b0a58-43bd-43df-a303-8e4e66aa377b"
      },
      "source": [
        "type(demo_df['essay'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jc82q5FkuPr3"
      },
      "source": [
        "content = \"Dear@CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many people have myspace, facebooks, aim, these all benefit us by having conversations with one another. Many people believe computers are bad but how can you make friends if you can never talk to them? I am very fortunate for having a computer that can help with not only school work but my social life and how I make friends. Computers help us with finding our locations, coordibates and millions of information online. If we didn't go on the internet a lot we wouldn't know how to go onto websites that @MONTH1 help us with locations and coordinates like @LOCATION1. Would you rather use a computer or be in @LOCATION3. When your supposed to be vacationing in @LOCATION2. Million of information is found on the internet. You can as almost every question and a computer will have it. Would you rather easily draw up a house plan on the computers or take @NUM1 hours doing one by hand with ugly erazer marks all over it, you are garrenteed that to find a job with a drawing like that. Also when appling for a job many workers must write very long papers like a @NUM3 word essay on why this job fits you the most, and many people I know don't like writing @NUM3 words non-stopp for hours when it could take them I hav an a computer. That is why computers we needed a lot now adays. I hope this essay has impacted your descion on computers because they are great machines to work with. The other day I showed my mom how to use a computer and she said it was the greatest invention sense sliced bread! Now go out and buy a computer to help you chat online with friends, find locations and millions of information on one click of the button and help your self with getting a job with neat, prepared, printed work that your boss will love.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yqdRloexuPr3"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "num_features = 300\n",
        "      \n",
        "model = KeyedVectors.load_word2vec_format( \"./word2vecmodel.bin\", binary=True)\n",
        "clean_test_essays = []\n",
        "clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n",
        "testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "testDataVecs = np.array(testDataVecs)\n",
        "testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "\n",
        "# lstm_model = get_model()\n",
        "lstm_model.load_weights(\"./final_lstm.h5\")\n",
        "preds = lstm_model.predict(testDataVecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFPD0IRVuPr4",
        "outputId": "9865cf3f-ea78-4413-83d6-a8a7ecfcb041"
      },
      "source": [
        "int(np.around(preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "49F7bofquPr4"
      },
      "source": [
        "# val_essays = demo_df['essay']\n",
        "# sentences = []\n",
        "\n",
        "# for essay in val_essays:\n",
        "#         sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
        "        \n",
        "\n",
        "# num_features = 300 \n",
        "# min_word_count = 40\n",
        "# num_workers = 4\n",
        "# contedft = 10\n",
        "# downsampling = 1e-3\n",
        "\n",
        "\n",
        "# model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = contedft, sample = downsampling)\n",
        "\n",
        "# model.init_sims(replace=True)\n",
        "\n",
        "# clean_train_essays = []\n",
        "\n",
        "# # Generate training and testing data word vectors.\n",
        "# for essay_v in val_essays:\n",
        "#     clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
        "# trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
        "\n",
        "# trainDataVecs = np.array(trainDataVecs)\n",
        "# # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
        "# trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
        "\n",
        "\n",
        "# y_pred = lstm_model.predict(trainDataVecs)\n",
        "\n",
        "\n",
        "\n",
        "# y_pred = np.around(y_pred)\n",
        "\n",
        "\n",
        "# # result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
        "# # print(\"Kappa Score: {}\".format(result))\n",
        "# print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQThsmQIuPr4",
        "outputId": "23d1ccfb-fd40-44d0-e96a-55942b106e44"
      },
      "source": [
        "y_test.T.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "LHmt5apQuPr4",
        "outputId": "fc33fd94-5416-40f8-b88c-d0957c21add4"
      },
      "source": [
        "y_pred1 = y_pred.T.reshape(2595,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-73cbcf22c0ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2595\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 109 into shape (2595,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "L8Rdh3wkuPr4"
      },
      "source": [
        "y_pred1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Br2oQ3MwuPr5"
      },
      "source": [
        "y_test1 = y_test.T.reshape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "prTEWpN2uPr5"
      },
      "source": [
        "from sklearn.metrics import confusion_matridf\n",
        "cm = confusion_matridf(y_test, y_pred1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UjLlOiCYuPr5"
      },
      "source": [
        "# Save a palette to a variable:\n",
        "palette = sns.color_palette(\"bright\")\n",
        " \n",
        "# Use palplot and pass in the variable:\n",
        "sns.palplot(palette)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HQyVBuuVuPr5"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "array =cm[0:10,0:10]\n",
        "df_cm = pd.DataFrame(array)\n",
        "plt.figure(figsize = (20,20))\n",
        "sns.heatmap(df_cm,cmap = 'Blues',square=True, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MQIpOXUduPr5"
      },
      "source": [
        "cm[0:10,0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RgODPL-cuPr5"
      },
      "source": [
        "arr = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
        "print(arr.shape)\n",
        "print(arr[1:4, 4:1])\n",
        "# print(arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qvpWcnifuPr5"
      },
      "source": [
        "cm.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tCgzFUjHuPr6"
      },
      "source": [
        "corr = cm\n",
        "\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, adf = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmadf=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "plt.title('Fig:1',size=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iynBbTGluPr6"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, y_pred1, average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "knGHitVtuPr6"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "cr = classification_report(y_test, y_pred1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EED4RFypuPr6"
      },
      "source": [
        "cr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "C7mb8LtMuPr6"
      },
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_classification_report(y_tru, y_prd, figsize=(10, 10), adf=None):\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    dfticks = ['precision', 'recall', 'f1-score', 'support']\n",
        "    yticks = list(np.unique(y_tru))\n",
        "    yticks += ['avg']\n",
        "\n",
        "    rep = np.array(precision_recall_fscore_support(y_tru, y_prd)).T\n",
        "    avg = np.mean(rep, adfis=0)\n",
        "    avg[-1] = np.sum(rep[:, -1])\n",
        "    rep = np.insert(rep, rep.shape[0], avg, adfis=0)\n",
        "\n",
        "    sns.heatmap(rep,\n",
        "                annot=True, \n",
        "                cbar=False, \n",
        "                dfticklabels=dfticks, \n",
        "                yticklabels=yticks,\n",
        "                adf=adf)\n",
        "\n",
        "plot_classification_report(y_test, y_pred1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VgIxWurbuPr6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "\n",
        "def plot_classification_report(classificationReport,\n",
        "                               title='Classification report',\n",
        "                               cmap='RdBu'):\n",
        "\n",
        "    classificationReport = classificationReport.replace('\\n\\n', '\\n')\n",
        "    classificationReport = classificationReport.replace(' / ', '/')\n",
        "    lines = classificationReport.split('\\n')\n",
        "\n",
        "    classes, plotMat, support, class_names = [], [], [], []\n",
        "    for line in lines[1:]:  # if you don't want avg/total result, then change [1:] into [1:-1]\n",
        "        t = line.strip().split()\n",
        "        if len(t) < 2:\n",
        "            continue\n",
        "        classes.append(t[0])\n",
        "        v = [float(df) for df in t[1: len(t) - 1]]\n",
        "        support.append(int(t[-1]))\n",
        "        class_names.append(t[0])\n",
        "        plotMat.append(v)\n",
        "\n",
        "    plotMat = np.array(plotMat)\n",
        "    dfticklabels = ['Precision', 'Recall', 'F1-score']\n",
        "    yticklabels = ['{0} ({1})'.format(class_names[iddf], sup)\n",
        "                   for iddf, sup in enumerate(support)]\n",
        "\n",
        "    plt.imshow(plotMat, interpolation='nearest', cmap=cmap, aspect='auto')\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    plt.dfticks(np.arange(3), dfticklabels, rotation=45)\n",
        "    plt.yticks(np.arange(len(classes)), yticklabels)\n",
        "\n",
        "    upper_thresh = plotMat.min() + (plotMat.madf() - plotMat.min()) / 10 * 8\n",
        "    lower_thresh = plotMat.min() + (plotMat.madf() - plotMat.min()) / 10 * 2\n",
        "    for i, j in itertools.product(range(plotMat.shape[0]), range(plotMat.shape[1])):\n",
        "        plt.tedft(j, i, format(plotMat[i, j], '.2f'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if (plotMat[i, j] > upper_thresh or plotMat[i, j] < lower_thresh) else \"black\")\n",
        "\n",
        "    plt.ylabel('Metrics')\n",
        "    plt.dflabel('Classes')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    sampleClassificationReport = cr\n",
        "    plot_classification_report(sampleClassificationReport)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J-aicucuuPr7"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report,confusion_matridf,accuracy_score,plot_confusion_matridf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6unoF5fOuPr7"
      },
      "source": [
        "classifiers = {\n",
        "    \"LogisiticRegression\": LogisticRegression(),\n",
        "    \"KNearest\": KNeighborsClassifier(n_neighbors=1),\n",
        "    \"Support Vector Classifier\": SVC(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
        "    \"MultinimialNB\": MultinomialNB()\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EnA370VvuPr7"
      },
      "source": [
        "trainDataVecs.shape[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KesxG5XCuPr7"
      },
      "source": [
        "trainDataVecs1 = np.reshape(trainDataVecs,trainDataVecs.shape[0],trainDataVecs.shape[2])\n",
        "trainDataVecs1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KYEXKBVYuPr7"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "classifier = KNeighborsClassifier()\n",
        "\n",
        "classifier.fit(trainDataVecs, y_train)\n",
        "training_score = cross_val_score(classifier, train_vectors, df_train[\"evaluator_rating\"], cv=5)\n",
        "print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XQK9rFiMuPr8"
      },
      "source": [
        "np.unique(df['evaluator_rating'],return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dfa9hzcouPr8"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XXuRIic_uPr8"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(df['evaluator_rating']),\n",
        "                                                 train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}